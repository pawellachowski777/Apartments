{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funkcja do pobierania danych tabelarycznych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adverts(pages=1):\n",
    "    '''\n",
    "    This function downloads tabular data about flats for rent in Warsaw from oxl.pl web side.\n",
    "    '''\n",
    "    \n",
    "    import bs4\n",
    "    from urllib.request import urlopen \n",
    "    from bs4 import BeautifulSoup as soup \n",
    "    import pandas as pd\n",
    "    \n",
    "    my_url = 'https://www.olx.pl/nieruchomosci/mieszkania/wynajem/warszawa/'\n",
    "    links = []\n",
    "    base = []\n",
    "\n",
    "    for i in range(pages+1):\n",
    "        \n",
    "        if pages == 0:\n",
    "            break\n",
    "            \n",
    "        elif i >= 1:\n",
    "            url = my_url + '?page=' + str(i)\n",
    "            uClient = urlopen(url)\n",
    "            page_html = uClient.read()\n",
    "            page_soup = soup(page_html, 'html.parser')\n",
    "            containers = page_soup.find_all('tr', {'class' : 'wrap'})\n",
    "            for cont in containers:\n",
    "                link = cont.tr.td.a['href']\n",
    "                if 'olx' in link:\n",
    "                    links.append(link)\n",
    "                    \n",
    "    rows = []\n",
    "    for link in list(set(links)):\n",
    "        try:\n",
    "            ad = urlopen(link).read()\n",
    "            ad_soup = soup(ad, features=\"html.parser\")\n",
    "            \n",
    "            added_class = ad_soup.find_all('li', {'class':'offer-bottombar__item'})\n",
    "            added = added_class[0].text.strip().split(', ')[1]\n",
    "            \n",
    "            localization_class = ad_soup.find_all('div', {'class':'offer-user__address'})\n",
    "            city = localization_class[0].text.split()[0]\n",
    "            district = localization_class[0].text.split()[2]\n",
    "\n",
    "            price_class = ad_soup.find_all('div', {'class':'pricelabel'})\n",
    "            price = price_class[0].text.strip().split('\\n')[0]\n",
    "            price = int(price.replace(' ', '').replace('zł',''))\n",
    "\n",
    "            by_class = ad_soup.find_all('ul', {'class':'offer-details'})\n",
    "            by = by_class[0].a.strong.text\n",
    "\n",
    "            level_class = ad_soup.find_all('li', {'class':'offer-details__item'})\n",
    "            level = level_class[1].a.text.strip().split('\\n')[1]\n",
    "\n",
    "            furniture_class = ad_soup.find_all('li', {'class':'offer-details__item'})\n",
    "            furniture = furniture_class[2].a.text.strip().replace('\\n', ' ').split()[1]\n",
    "            furniture\n",
    "            if furniture == 'Tak':\n",
    "                furniture = 1\n",
    "            else:\n",
    "                furniture = 0\n",
    "\n",
    "            building_class = ad_soup.find_all('li', {'class':'offer-details__item'})\n",
    "            building = building_class[3].a.text.split()[2]\n",
    "\n",
    "            surface_class = ad_soup.find_all('li', {'class':'offer-details__item'})\n",
    "            surface = int(surface_class[4].text.strip().replace('\\n', ' ').split()[1])\n",
    "\n",
    "            rooms_class = ad_soup.find_all('li', {'class':'offer-details__item'})\n",
    "            rooms = rooms_class[5].text.strip().replace('\\n', ' ').split()[2]\n",
    "\n",
    "            rent_class = ad_soup.find_all('li', {'class':'offer-details__item'})\n",
    "            rent = int(rent_class[6].text.strip().replace('\\n', ' ').split()[2])\n",
    "            \n",
    "            rows.append([link, city, district, price, by, level, furniture, \n",
    "                         building, surface, rooms, rent])\n",
    "        except Exception:\n",
    "            pass\n",
    "    print('pobrano dane z', len(rows), 'ogłoszeń.')\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01/02/2021 18:21:32\n",
      "pobrano dane z 37 ogłoszeń.\n"
     ]
    }
   ],
   "source": [
    "# from datetime import datetime\n",
    "# now = datetime.now()\n",
    "# print(now.strftime('%d/%m/%Y %H:%M:%S'))\n",
    "\n",
    "# rows = adverts(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funkcja automatycznie dodająca nowe wiersze do istiejącego arkusza Excela (skopiowane ze StackOverflow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def append_df_to_excel(filename, df, sheet_name='Sheet1', startrow=None,\n",
    "                       truncate_sheet=False, \n",
    "                       **to_excel_kwargs):\n",
    "    \"\"\"\n",
    "    Append a DataFrame [df] to existing Excel file [filename]\n",
    "    into [sheet_name] Sheet.\n",
    "    If [filename] doesn't exist, then this function will create it.\n",
    "\n",
    "    Parameters:\n",
    "      filename : File path or existing ExcelWriter\n",
    "                 (Example: '/path/to/file.xlsx')\n",
    "      df : dataframe to save to workbook\n",
    "      sheet_name : Name of sheet which will contain DataFrame.\n",
    "                   (default: 'Sheet1')\n",
    "      startrow : upper left cell row to dump data frame.\n",
    "                 Per default (startrow=None) calculate the last row\n",
    "                 in the existing DF and write to the next row...\n",
    "      truncate_sheet : truncate (remove and recreate) [sheet_name]\n",
    "                       before writing DataFrame to Excel file\n",
    "      to_excel_kwargs : arguments which will be passed to `DataFrame.to_excel()`\n",
    "                        [can be dictionary]\n",
    "\n",
    "    Returns: None\n",
    "\n",
    "    (c) [MaxU](https://stackoverflow.com/users/5741205/maxu?tab=profile)\n",
    "    \"\"\"\n",
    "    from openpyxl import load_workbook\n",
    "\n",
    "    # ignore [engine] parameter if it was passed\n",
    "    if 'engine' in to_excel_kwargs:\n",
    "        to_excel_kwargs.pop('engine')\n",
    "\n",
    "    writer = pd.ExcelWriter(filename, engine='openpyxl')\n",
    "\n",
    "    # Python 2.x: define [FileNotFoundError] exception if it doesn't exist \n",
    "    try:\n",
    "        FileNotFoundError\n",
    "    except NameError:\n",
    "        FileNotFoundError = IOError\n",
    "\n",
    "\n",
    "    try:\n",
    "        # try to open an existing workbook\n",
    "        writer.book = load_workbook(filename)\n",
    "        \n",
    "        # get the last row in the existing Excel sheet\n",
    "        # if it was not specified explicitly\n",
    "        if startrow is None and sheet_name in writer.book.sheetnames:\n",
    "            startrow = writer.book[sheet_name].max_row\n",
    "\n",
    "        # truncate sheet\n",
    "        if truncate_sheet and sheet_name in writer.book.sheetnames:\n",
    "            # index of [sheet_name] sheet\n",
    "            idx = writer.book.sheetnames.index(sheet_name)\n",
    "            # remove [sheet_name]\n",
    "            writer.book.remove(writer.book.worksheets[idx])\n",
    "            # create an empty sheet [sheet_name] using old index\n",
    "            writer.book.create_sheet(sheet_name, idx)\n",
    "        \n",
    "        # copy existing sheets\n",
    "        writer.sheets = {ws.title:ws for ws in writer.book.worksheets}\n",
    "    except FileNotFoundError:\n",
    "        # file does not exist yet, we will create it\n",
    "        pass\n",
    "\n",
    "    if startrow is None:\n",
    "        startrow = 0\n",
    "\n",
    "    # write out the new sheet\n",
    "    df.to_excel(writer, sheet_name, startrow=startrow, **to_excel_kwargs)\n",
    "\n",
    "    # save the workbook\n",
    "    writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "base = pd.DataFrame(rows, \n",
    "                    columns = ['link', 'miasto', 'dzielnica', 'cena', 'od', 'poziom', \n",
    "                               'umeblowanie','zabudowa','powierzchnia', 'pokoje', 'czynsz dodatkowo'])\n",
    "    \n",
    "append_df_to_excel('excel2.xlsx', base, index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funkcja do pobierania danych ze wskazanych dzielnic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adverts_districsts(districts, pages=1, text=False):\n",
    "    '''\n",
    "    districts: list, at least one with the following:\n",
    "        Bemowo, Białołęka, Bielany, Mokotów, Ochota, Praga Południe, Praga Północ, Rembertów, Śródmieście,\n",
    "        Targówek, Ursus, Ursynów, Wawer, Wesoła, Wilanów, Włochy, Wola, Żoliborz\n",
    "    '''\n",
    "    import bs4\n",
    "    from urllib.request import urlopen \n",
    "    from bs4 import BeautifulSoup as soup \n",
    "    import pandas as pd\n",
    "\n",
    "    my_url = 'https://www.olx.pl/nieruchomosci/mieszkania/wynajem/warszawa/'\n",
    "    links = []\n",
    "    rows = []\n",
    "    base = []\n",
    "    district_dict = dict([\n",
    "        ('367', 'Bemowo'), ('365', 'Białołęka'), ('369', 'Bielany'), ('353', 'Mokotów'), ('355', 'Ochota'),\n",
    "        ('381', 'Praga Południe'), ('379', 'Praga Północ'), ('361', 'Rembertów'), ('351', 'Śródmieście'),\n",
    "        ('377', 'Targówek'), ('371', 'Ursus'), ('373', 'Ursynów'), ('383', 'Wawer'), ('533', 'Wesoła'),\n",
    "        ('375', 'Wilanów'), ('357', 'Włochy'), ('359', 'Wola'), ('363', 'Żoliborz')\n",
    "    ]) # słownik dzielnic i odopiwadających im kluczy id na stronie\n",
    "    keys = [key for key, value in district_dict.items() if value in districts] # lista kluczy wybranych w funkcji dzielnic\n",
    "    \n",
    "    for k in keys:\n",
    "        for i in range(pages+1):\n",
    "            if pages == 0:\n",
    "                break\n",
    "\n",
    "            elif i >= 1:\n",
    "                url = my_url + '?search%5Bdistrict_id%5D=' + str(k) + '&page=' + str(i)\n",
    "                if str(k) in url:\n",
    "                    uClient = urlopen(url)\n",
    "                    page_html = uClient.read()\n",
    "                    page_soup = soup(page_html, 'html.parser')\n",
    "                    containers = page_soup.find_all('tr', {'class' : 'wrap'})\n",
    "                    for cont in containers:\n",
    "                        link = cont.tr.td.a['href']\n",
    "                        if 'olx' in link:\n",
    "                            links.append(link)\n",
    "\n",
    "    for link in list(set(links)):\n",
    "        try:\n",
    "            ad = urlopen(link).read()\n",
    "            ad_soup = soup(ad)\n",
    "\n",
    "            added_class = ad_soup.find_all('li', {'class':'offer-bottombar__item'})\n",
    "            added = added_class[0].text.strip().split(', ')[1]\n",
    "\n",
    "            localization_class = ad_soup.find_all('div', {'class':'offer-user__address'})\n",
    "            city = localization_class[0].text.split()[0]\n",
    "            district = localization_class[0].text.split()[2]\n",
    "\n",
    "            price_class = ad_soup.find_all('div', {'class':'pricelabel'})\n",
    "            price = price_class[0].text.strip().split('\\n')[0]\n",
    "            price = int(price.replace(' ', '').replace('zł',''))\n",
    "\n",
    "            by_class = ad_soup.find_all('ul', {'class':'offer-details'})\n",
    "            by = by_class[0].a.strong.text\n",
    "\n",
    "            level_class = ad_soup.find_all('li', {'class':'offer-details__item'})\n",
    "            level = level_class[1].a.text.strip().split('\\n')[1]\n",
    "\n",
    "            furniture_class = ad_soup.find_all('li', {'class':'offer-details__item'})\n",
    "            furniture = furniture_class[2].a.text.strip().replace('\\n', ' ').split()[1]\n",
    "            furniture\n",
    "            if furniture == 'Tak':\n",
    "                furniture = 1\n",
    "            else:\n",
    "                furniture = 0\n",
    "\n",
    "            building_class = ad_soup.find_all('li', {'class':'offer-details__item'})\n",
    "            building = building_class[3].a.text.split()[2]\n",
    "\n",
    "            surface_class = ad_soup.find_all('li', {'class':'offer-details__item'})\n",
    "            surface = int(surface_class[4].text.strip().replace('\\n', ' ').split()[1])\n",
    "\n",
    "            rooms_class = ad_soup.find_all('li', {'class':'offer-details__item'})\n",
    "            rooms = rooms_class[5].text.strip().replace('\\n', ' ').split()[2]\n",
    "\n",
    "            rent_class = ad_soup.find_all('li', {'class':'offer-details__item'})\n",
    "            rent = int(rent_class[6].text.strip().replace('\\n', ' ').split()[2])\n",
    "            \n",
    "            title_class = ad_soup.find_all('div', {'class':'offer-titlebox'})\n",
    "            title = title_class[0].h1.text.strip() \n",
    "            \n",
    "            description_class = ad_soup.find_all('div', {'class':'clr lheight20 large'})\n",
    "            description = description_class[0].text.strip().replace('\\r\\n', ' ')\n",
    "            \n",
    "            if text == False:\n",
    "                rows.append([link, city, district, price, by, level, furniture, \n",
    "                             building, surface, rooms, rent])\n",
    "            else:\n",
    "                rows.append([district, price, by, level, building,\n",
    "                             surface, rent, title, description])\n",
    "            \n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    print('pobrano dane z', len(rows), 'ogłoszeń.')\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/01/2021 11:57:43\n",
      "pobrano dane z 145 ogłoszeń.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "print(now.strftime('%d/%m/%Y %H:%M:%S'))\n",
    "\n",
    "rows = adverts_districsts(districts=['Wesoła', 'Rembertów', 'Wawer', 'Wilanów', 'Ursus',], pages=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "base = pd.DataFrame(rows, \n",
    "                    columns = ['link', 'miasto', 'dzielnica', 'cena', 'od', 'poziom', \n",
    "                               'umeblowanie','zabudowa','powierzchnia', 'pokoje', 'czynsz dodatkowo'])\n",
    "#append_df_to_excel(filename='ogloszenia.xlsx', df=base, sheet_name='dzielnice', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wawer        31\n",
       "Ursus        31\n",
       "Rembertów    29\n",
       "Wesoła       27\n",
       "Włochy       27\n",
       "Name: dzielnica, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base['dzielnica'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funkcja pobierające opisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adverts_text(pages=1):\n",
    "    \n",
    "    import bs4\n",
    "    from urllib.request import urlopen \n",
    "    from bs4 import BeautifulSoup as soup \n",
    "    import pandas as pd\n",
    "    \n",
    "    my_url = 'https://www.olx.pl/nieruchomosci/mieszkania/wynajem/warszawa/'\n",
    "    links = []\n",
    "    base = []\n",
    "\n",
    "    for i in range(pages+1):\n",
    "        \n",
    "        if pages == 0:\n",
    "            break\n",
    "            \n",
    "        elif i >= 1:\n",
    "            url = my_url + '?page=' + str(i)\n",
    "            uClient = urlopen(url)\n",
    "            page_html = uClient.read()\n",
    "            page_soup = soup(page_html, 'html.parser')\n",
    "            containers = page_soup.find_all('tr', {'class' : 'wrap'})\n",
    "            for cont in containers:\n",
    "                link = cont.tr.td.a['href']\n",
    "                if 'olx' in link:\n",
    "                    links.append(link)\n",
    "                    \n",
    "    rows = []\n",
    "    for link in list(set(links)):\n",
    "        try:\n",
    "            ad = urlopen(link).read()\n",
    "            ad_soup = soup(ad)\n",
    "            \n",
    "            localization_class = ad_soup.find_all('div', {'class':'offer-user__address'})\n",
    "            district = localization_class[0].text.split()[2]\n",
    "\n",
    "            price_class = ad_soup.find_all('div', {'class':'pricelabel'})\n",
    "            price = price_class[0].text.strip().split('\\n')[0]\n",
    "            price = int(price.replace(' ', '').replace('zł',''))\n",
    "\n",
    "            by_class = ad_soup.find_all('ul', {'class':'offer-details'})\n",
    "            by = by_class[0].a.strong.text\n",
    "\n",
    "            level_class = ad_soup.find_all('li', {'class':'offer-details__item'})\n",
    "            level = level_class[1].a.text.strip().split('\\n')[1]\n",
    "\n",
    "            building_class = ad_soup.find_all('li', {'class':'offer-details__item'})\n",
    "            building = building_class[3].a.text.split()[2]\n",
    "\n",
    "            surface_class = ad_soup.find_all('li', {'class':'offer-details__item'})\n",
    "            surface = int(surface_class[4].text.strip().replace('\\n', ' ').split()[1])\n",
    "\n",
    "            rent_class = ad_soup.find_all('li', {'class':'offer-details__item'})\n",
    "            rent = int(rent_class[6].text.strip().replace('\\n', ' ').split()[2])\n",
    "            \n",
    "            # dane tekstowe\n",
    "            title_class = ad_soup.find_all('div', {'class':'offer-titlebox'})\n",
    "            title = title_class[0].h1.text.strip() \n",
    "            \n",
    "            description_class = ad_soup.find_all('div', {'class':'clr lheight20 large'})\n",
    "            description = description_class[0].text.strip().replace('\\r\\n', ' ')\n",
    "            \n",
    "            rows.append([district, price, by, level, building,\n",
    "                         surface, rent, title, description])\n",
    "        except Exception:\n",
    "            pass\n",
    "    print('pobrano dane z', len(rows), 'ogłoszeń.')\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pobrano dane z 30 ogłoszeń.\n"
     ]
    }
   ],
   "source": [
    "rows = adverts() # 756 ogłoszeń z dnia 03.01.2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "base = pd.DataFrame(rows, \n",
    "                    columns = ['dzielnica', 'cena', 'od', 'poziom', 'zabudowa',\n",
    "                               'powierzchnia', 'czynsz dodatkowo', 'tytuł', 'opis'])\n",
    "base.drop_duplicates(inplace=True)\n",
    "base['cena całkowita'] = base['cena'] + base['czynsz dodatkowo']\n",
    "base = base.drop(['cena', 'czynsz dodatkowo'], axis=1)\n",
    "\n",
    "base.to_excel('data_text.xlsx', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pobrano dane z 350 ogłoszeń.\n"
     ]
    }
   ],
   "source": [
    "rows = adverts_districsts(districts=['Bemowo', 'Białołęka', 'Bielany', 'Ochota', 'Rembertów', 'Targówek', \n",
    "                              'Ursus', 'Ursynów', 'Wawer', 'Wesoła', 'Włochy', 'Żoliborz'],\n",
    "                   text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Włochy       33\n",
      "Targówek     31\n",
      "Żoliborz     31\n",
      "Bemowo       31\n",
      "Rembertów    31\n",
      "Białołęka    30\n",
      "Ursynów      29\n",
      "Wawer        29\n",
      "Bielany      28\n",
      "Ochota       27\n",
      "Wesoła       26\n",
      "Ursus        24\n",
      "Name: dzielnica, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "base = pd.DataFrame(rows, \n",
    "                    columns = ['dzielnica', 'cena', 'od', 'poziom', 'zabudowa',\n",
    "                               'powierzchnia', 'czynsz dodatkowo', 'tytuł', 'opis'])\n",
    "print(base['dzielnica'].value_counts())\n",
    "base['cena całkowita'] = base['cena'] + base['czynsz dodatkowo']\n",
    "base = base.drop(['cena', 'czynsz dodatkowo'], axis=1)\n",
    "append_df_to_excel(r'E:\\python\\projects\\apartments\\data\\data_text.xlsx', base, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pobrano dane z 48 ogłoszeń.\n"
     ]
    }
   ],
   "source": [
    "rows = adverts_districsts(districts=['Praga Północ','Wilanów'],text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Praga-Północ    25\n",
      "Wilanów         23\n",
      "Name: dzielnica, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "base = pd.DataFrame(rows, \n",
    "                    columns = ['dzielnica', 'cena', 'od', 'poziom', 'zabudowa',\n",
    "                               'powierzchnia', 'czynsz dodatkowo', 'tytuł', 'opis'])\n",
    "print(base['dzielnica'].value_counts())\n",
    "base['cena całkowita'] = base['cena'] + base['czynsz dodatkowo']\n",
    "base = base.drop(['cena', 'czynsz dodatkowo'], axis=1)\n",
    "append_df_to_excel(r'E:\\python\\projects\\apartments\\data\\data_text.xlsx', base, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
